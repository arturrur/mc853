{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturrur/mc853/blob/using-KNN-imputer/fun%C3%A7%C3%B5es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a28f6cca-9c5e-4228-9248-dc682e75a670",
      "metadata": {
        "id": "a28f6cca-9c5e-4228-9248-dc682e75a670"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import pandas as pd                       # For data manipulation and analysis\n",
        "import numpy as np                        # For numerical computing\n",
        "import time                               # For tracking time\n",
        "import math                               # For mathematical operations\n",
        "import warnings                           # For managing warnings\n",
        "\n",
        "import shap                               # For SHAP (SHapley Additive exPlanations) values\n",
        "\n",
        "import imblearn                           # For dealing with imbalanced datasets\n",
        "from imblearn.over_sampling import RandomOverSampler   # For oversampling\n",
        "from imblearn.under_sampling import RandomUnderSampler # For undersampling\n",
        "\n",
        "\n",
        "import seaborn as sns                     # For statistical data visualization\n",
        "import matplotlib.pyplot as plt           # For creating visualizations\n",
        "import matplotlib.patches as mpatches     # For drawing patches in plots\n",
        "import matplotlib.colors as mcolors       # For defining custom colors in plots\n",
        "import matplotlib.ticker as ticker        # For formatting tick marks on plots\n",
        "from matplotlib.ticker import FuncFormatter         # For custom tick formatting\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler      # For feature scaling\n",
        "from sklearn.model_selection import (StratifiedKFold) # For splitting data into train and test sets\n",
        "\n",
        "from sklearn.metrics import (roc_auc_score,           # For evaluating model performance\n",
        "                             recall_score)\n",
        "\n",
        "from sklearn.svm import SVC                           # For Support Vector Classifier\n",
        "from sklearn.linear_model import LogisticRegression   # For Logistic Regression Classifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, # For ensemble classifiers\n",
        "                              GradientBoostingClassifier,\n",
        "                              BaggingClassifier)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier      # For Multi-layer Perceptron Classifier\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "import re\n",
        "from sklearn.calibration import CalibratedClassifierCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "67ebf0c6-b108-4e25-9a3a-2da7f2717f6a",
      "metadata": {
        "id": "67ebf0c6-b108-4e25-9a3a-2da7f2717f6a"
      },
      "outputs": [],
      "source": [
        "# Set the number of folds for cross-validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
        "\n",
        "# Set preprocessing: StandardScaler for feature standardization\n",
        "preprocessing = StandardScaler()\n",
        "\n",
        "# Initialize KNNImputer with the specified number of neighbors\n",
        "imputer = KNNImputer(n_neighbors=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ff5a01e1-3ee8-4e70-b1ed-71d1795095ae",
      "metadata": {
        "id": "ff5a01e1-3ee8-4e70-b1ed-71d1795095ae"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary containing various classification algorithms\n",
        "\n",
        "algorithms = {\n",
        "    'svc_linear': SVC(probability=True, kernel='linear', random_state=0),\n",
        "    # Support Vector Classifier with linear kernel\n",
        "\n",
        "    'svc_rbf': SVC(probability=True, kernel='rbf', random_state=0),\n",
        "    # Support Vector Classifier with radial basis function (RBF) kernel\n",
        "\n",
        "    'random_forest': RandomForestClassifier(random_state=0),\n",
        "    # Random Forest Classifier\n",
        "\n",
        "    'gradient_boosting': GradientBoostingClassifier(random_state=0),\n",
        "    # Gradient Boosting Classifier\n",
        "\n",
        "    'logistic_regression': LogisticRegression(),\n",
        "    # Logistic Regression Classifier\n",
        "\n",
        "    'bagging': BaggingClassifier(random_state=0),\n",
        "    # Bagging Classifier\n",
        "\n",
        "    'mlp': MLPClassifier(random_state=0)\n",
        "    # Multi-layer Perceptron Classifier\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1a580b6-211d-4a4d-85a0-e62afb275421",
      "metadata": {
        "id": "a1a580b6-211d-4a4d-85a0-e62afb275421"
      },
      "outputs": [],
      "source": [
        "def data_sample(X, y):\n",
        "    '''\n",
        "    Receives a set of features and target feature separately.\n",
        "    Returns balanced data, with the same number of samples in both classes.\n",
        "    If the minority class is less than 5%, applies oversampling and undersampling.\n",
        "    Otherwise, applies only undersampling.\n",
        "    Parameters:\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The feature matrix.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The target variable.\n",
        "\n",
        "    Returns:\n",
        "        X_resampled : array-like, shape (n_samples_resampled, n_features)\n",
        "            The resampled feature matrix.\n",
        "        y_resampled : array-like, shape (n_samples_resampled,)\n",
        "            The resampled target variable.\n",
        "    '''\n",
        "\n",
        "    # Define sampling strategies\n",
        "    undersample = RandomUnderSampler(sampling_strategy='majority', random_state=1)\n",
        "    oversample = RandomOverSampler(sampling_strategy=0.2, random_state=1)\n",
        "\n",
        "    # Identify the minority class\n",
        "    count_1 = (y == 1).sum()\n",
        "    count_0 = (y == 0).sum()\n",
        "    count_min = min(count_0, count_1)\n",
        "    count_max = max(count_0, count_1)\n",
        "    print(f\"count_min: {count_min}\")\n",
        "    print(f\"count_max: {count_max}\")\n",
        "\n",
        "    # Calculate the percentage of the minority class compared to the total number of instances\n",
        "    ratio = (count_min / count_max)\n",
        "\n",
        "    # If the minority class is more than 60% of the majority class, do not apply any resampling technique\n",
        "    if ratio > 0.6:\n",
        "        X_resampled, y_resampled = X, y\n",
        "\n",
        "    # Check if the percentage of class 1 is at least 5% of the total number of instances\n",
        "    # If it is less than 5%, apply both over and under sampling\n",
        "    else:\n",
        "        if ratio <= 0.2:\n",
        "            X_resampled, y_resampled = oversample.fit_resample(X, y)\n",
        "            X_resampled, y_resampled = undersample.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "        # Otherwise, apply only undersampling\n",
        "        else:\n",
        "            X_resampled, y_resampled = undersample.fit_resample(X, y)\n",
        "\n",
        "    return X_resampled, y_resampled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7192dcb9-be8b-41ed-ac10-9302fad1a7a6",
      "metadata": {
        "id": "7192dcb9-be8b-41ed-ac10-9302fad1a7a6"
      },
      "outputs": [],
      "source": [
        "def impute_missing(data, n_neighbors=3):\n",
        "    \"\"\"\n",
        "    Impute missing values using the K-nearest neighbors algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        data (pd.DataFrame): Input DataFrame with missing values.\n",
        "        n_neighbors (int, optional): Number of neighbors to use for imputation. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with missing values imputed using KNN.\n",
        "    \"\"\"\n",
        "    # Initialize KNNImputer with the specified number of neighbors\n",
        "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "\n",
        "    # Perform imputation\n",
        "    imputed_data = imputer.fit_transform(data)\n",
        "\n",
        "    # Convert the imputed array back to a DataFrame\n",
        "    imputed_df = pd.DataFrame(imputed_data, columns=data.columns, index=data.index)\n",
        "\n",
        "    return imputed_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bfe8f23f-9dd9-4e7f-9844-7dd573137f53",
      "metadata": {
        "id": "bfe8f23f-9dd9-4e7f-9844-7dd573137f53"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "def evaluate_cv(data):\n",
        "    '''\n",
        "    Receives data to be evaluated and returns the average performance inside cross-validation, using 3 metrics.\n",
        "    Applies over-under sampling to get balanced datasets and standardizes features.\n",
        "\n",
        "    Parameters:\n",
        "    data : DataFrame\n",
        "        The dataset containing features and the target variable.\n",
        "\n",
        "    Returns:\n",
        "    df : DataFrame\n",
        "        A DataFrame containing the mean and standard deviation of each algorithm's performance across 5-fold cross-validation.\n",
        "        The performance metrics include AUC (mean and standard deviation), sensitivity (mean and standard deviation),\n",
        "        specificity (mean and standard deviation), prec_n (mean and standard deviation), and prec_p (mean and standard deviation).\n",
        "    '''\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Identify the target column\n",
        "    target_feature = data.columns[-1]\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = data.drop(columns=[target_feature])\n",
        "    y = data[target_feature]\n",
        "\n",
        "    # Initialize dictionaries to store metrics for each algorithm\n",
        "    sen = {}\n",
        "    spe = {}\n",
        "    auc = {}\n",
        "    prec_n = {}  # Negative precision\n",
        "    prec_p = {}  # Positive precision\n",
        "\n",
        "    for algorithm in algorithms.keys():\n",
        "        sen[algorithm] = []\n",
        "        spe[algorithm] = []\n",
        "        auc[algorithm] = []\n",
        "        prec_n[algorithm] = []\n",
        "        prec_p[algorithm] = []\n",
        "\n",
        "    # Iterate through each round of the cross-validation\n",
        "    for train, test in kf.split(X, y):\n",
        "        # Allocate train and test data\n",
        "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "        y_train, y_test = y.iloc[train], y.iloc[test]\n",
        "\n",
        "        # Apply over-under sampling\n",
        "        X_train, y_train = data_sample(X_train, y_train)\n",
        "\n",
        "        X_train = imputer.fit_transform(X_train)\n",
        "        X_test = imputer.transform(X_test)\n",
        "\n",
        "        # Standardize features\n",
        "        X_train = preprocessing.fit_transform(X_train)\n",
        "        X_test = preprocessing.transform(X_test)\n",
        "\n",
        "        # Iterate through each algorithm\n",
        "        for algorithm, (clf) in algorithms.items():\n",
        "\n",
        "            clf.fit((X_train), y_train)\n",
        "\n",
        "            # Make predictions for the test data\n",
        "            y_pred = clf.predict(X_test)\n",
        "\n",
        "            # Calculate sensitivity and specificity\n",
        "            recallscore = recall_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "            sen[algorithm].append(recallscore[1])\n",
        "            spe[algorithm].append(recallscore[0])\n",
        "\n",
        "            # Calculate precision for each class\n",
        "            prec_score = precision_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "            prec_n[algorithm].append(prec_score[0])\n",
        "            prec_p[algorithm].append(prec_score[1])\n",
        "\n",
        "            # Calculate the area under the ROC curve\n",
        "            aucscore = roc_auc_score(y_test, (clf.predict_proba((X_test)))[:, 1])\n",
        "            auc[algorithm].append(aucscore)\n",
        "\n",
        "    # Create a DataFrame with the mean and standard deviation of each algorithm's performance across 5 folds\n",
        "    df = pd.DataFrame(columns=list(algorithms.keys()))\n",
        "\n",
        "    df.loc['auc (mean)'] = [np.mean(auc['svc_linear']), np.mean(auc['svc_rbf']), np.mean(auc['random_forest']),\n",
        "                            np.mean(auc['gradient_boosting']), np.mean(auc['logistic_regression']),\n",
        "                            np.mean(auc['bagging']), np.mean(auc['mlp'])]\n",
        "\n",
        "    df.loc['auc (stdev)'] = [np.std(auc['svc_linear']), np.std(auc['svc_rbf']), np.std(auc['random_forest']),\n",
        "                             np.std(auc['gradient_boosting']), np.std(auc['logistic_regression']),\n",
        "                             np.std(auc['bagging']), np.std(auc['mlp'])]\n",
        "\n",
        "    df.loc['rcl_1 (mean)'] = [np.mean(sen['svc_linear']), np.mean(sen['svc_rbf']), np.mean(sen['random_forest']),\n",
        "                            np.mean(sen['gradient_boosting']), np.mean(sen['logistic_regression']),\n",
        "                            np.mean(sen['bagging']), np.mean(sen['mlp'])]\n",
        "\n",
        "    df.loc['rcl_1 (stdev)'] = [np.std(sen['svc_linear']), np.std(sen['svc_rbf']), np.std(sen['random_forest']),\n",
        "                             np.std(sen['gradient_boosting']), np.std(sen['logistic_regression']),\n",
        "                             np.std(sen['bagging']), np.std(sen['mlp'])]\n",
        "\n",
        "    df.loc['rcl_0 (mean)'] = [np.mean(spe['svc_linear']), np.mean(spe['svc_rbf']), np.mean(spe['random_forest']),\n",
        "                            np.mean(spe['gradient_boosting']), np.mean(spe['logistic_regression']),\n",
        "                            np.mean(spe['bagging']), np.mean(spe['mlp'])]\n",
        "\n",
        "    df.loc['rcl_0 (stdev)'] = [np.std(spe['svc_linear']), np.std(spe['svc_rbf']), np.std(spe['random_forest']),\n",
        "                             np.std(spe['gradient_boosting']), np.std(spe['logistic_regression']),\n",
        "                             np.std(spe['bagging']), np.std(spe['mlp'])]\n",
        "\n",
        "    df.loc['prc_1 (mean)'] = [np.mean(prec_p['svc_linear']), np.mean(prec_p['svc_rbf']), np.mean(prec_p['random_forest']),\n",
        "                                 np.mean(prec_p['gradient_boosting']), np.mean(prec_p['logistic_regression']),\n",
        "                                 np.mean(prec_p['bagging']), np.mean(prec_p['mlp'])]\n",
        "\n",
        "    df.loc['prc_1 (stdev)'] = [np.std(prec_p['svc_linear']), np.std(prec_p['svc_rbf']), np.std(prec_p['random_forest']),\n",
        "                                  np.std(prec_p['gradient_boosting']), np.std(prec_p['logistic_regression']),\n",
        "                                  np.std(prec_p['bagging']), np.std(prec_p['mlp'])]\n",
        "\n",
        "    df.loc['prc_0 (mean)'] = [np.mean(prec_n['svc_linear']), np.mean(prec_n['svc_rbf']), np.mean(prec_n['random_forest']),\n",
        "                                 np.mean(prec_n['gradient_boosting']), np.mean(prec_n['logistic_regression']),\n",
        "                                 np.mean(prec_n['bagging']), np.mean(prec_n['mlp'])]\n",
        "\n",
        "    df.loc['prc_0 (stdev)'] = [np.std(prec_n['svc_linear']), np.std(prec_n['svc_rbf']), np.std(prec_n['random_forest']),\n",
        "                                  np.std(prec_n['gradient_boosting']), np.std(prec_n['logistic_regression']),\n",
        "                                  np.std(prec_n['bagging']), np.std(prec_n['mlp'])]\n",
        "\n",
        "    # Set caption for DataFrame\n",
        "    df = df.style.set_caption('Average performance and standard deviation among 5-fold cross-validation')\n",
        "\n",
        "    # Record the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(df)\n",
        "\n",
        "    # Print the total time taken to run cross-validation\n",
        "    print(f\"Total time taken to run cross-validation: {total_time:.2f} seconds\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e691d3fe-206c-4b5b-ad01-c57388737cf3",
      "metadata": {
        "id": "e691d3fe-206c-4b5b-ad01-c57388737cf3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "def evaluate_external(data, data_test):\n",
        "    '''\n",
        "    Receives data and data_test to be evaluated and returns the average performance, using 3 metrics.\n",
        "    Applies over-under sampling to get balanced datasets and standardizes features.\n",
        "\n",
        "    Parameters:\n",
        "    data : DataFrame\n",
        "        The training dataset containing features and the target variable.\n",
        "    data_test : DataFrame\n",
        "        The test dataset containing features and the target variable.\n",
        "\n",
        "    Returns:\n",
        "    df : DataFrame\n",
        "        A DataFrame containing the mean performance of each algorithm across external validation.\n",
        "        The performance metrics include AUC (mean), sensitivity (mean), specificity (mean), prec_n (mean), and prec_p (mean).\n",
        "    '''\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Identify the target column\n",
        "    target_feature = data.columns[-1]\n",
        "\n",
        "    # Separate features (X) and target (y) for training data\n",
        "    X = data.drop(columns=[target_feature])\n",
        "    y = data[target_feature]\n",
        "\n",
        "    # Separate features (X) and target (y) for test data\n",
        "    X_test = data_test.drop(columns=[target_feature])\n",
        "    y_test = data_test[target_feature]\n",
        "\n",
        "    # Initialize dictionaries to store metrics for each algorithm\n",
        "    sen = {}\n",
        "    spe = {}\n",
        "    auc = {}\n",
        "    prec_n = {}  # Negative precision\n",
        "    prec_p = {}  # Positive precision\n",
        "\n",
        "    for algorithm in algorithms.keys():\n",
        "        sen[algorithm] = []\n",
        "        spe[algorithm] = []\n",
        "        auc[algorithm] = []\n",
        "        prec_n[algorithm] = []\n",
        "        prec_p[algorithm] = []\n",
        "\n",
        "    # Apply over-under sampling to training data\n",
        "    X_train, y_train = data_sample(X, y)\n",
        "    #X_train = X\n",
        "    #y_train = y\n",
        "\n",
        "    X_train = imputer.fit_transform(X_train)\n",
        "    X_test = imputer.transform(X_test)\n",
        "\n",
        "    # Apply preprocessing to both training and test data\n",
        "    X_train = preprocessing.fit_transform(X_train)\n",
        "    X_test = preprocessing.transform(X_test)\n",
        "\n",
        "    # For each algorithm\n",
        "    for algorithm, (clf) in algorithms.items():\n",
        "        # Train model\n",
        "        clf.fit((X_train), y_train)\n",
        "\n",
        "        # Make predictions for the test data\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        # Calculate sensitivity and specificity\n",
        "        recallscore = recall_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "        sen[algorithm].append(recallscore[1])\n",
        "        spe[algorithm].append(recallscore[0])\n",
        "\n",
        "        # Calculate precision for each class\n",
        "        prec_score = precision_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "        prec_n[algorithm].append(prec_score[0])\n",
        "        prec_p[algorithm].append(prec_score[1])\n",
        "\n",
        "        # Calculate the area under the ROC curve\n",
        "        aucscore = roc_auc_score(y_test, (clf.predict_proba((X_test)))[:, 1])\n",
        "        auc[algorithm].append(aucscore)\n",
        "\n",
        "    # Create a DataFrame with the mean performance of each algorithm across the external validation\n",
        "    df = pd.DataFrame(columns=list(algorithms.keys()))\n",
        "\n",
        "    df.loc['auc'] = [np.mean(auc['svc_linear']), np.mean(auc['svc_rbf']), np.mean(auc['random_forest']),\n",
        "                            np.mean(auc['gradient_boosting']), np.mean(auc['logistic_regression']),\n",
        "                            np.mean(auc['bagging']), np.mean(auc['mlp'])]\n",
        "\n",
        "    df.loc['rcl_1'] = [np.mean(sen['svc_linear']), np.mean(sen['svc_rbf']), np.mean(sen['random_forest']),\n",
        "                            np.mean(sen['gradient_boosting']), np.mean(sen['logistic_regression']),\n",
        "                            np.mean(sen['bagging']), np.mean(sen['mlp'])]\n",
        "\n",
        "    df.loc['rcl_0'] = [np.mean(spe['svc_linear']), np.mean(spe['svc_rbf']), np.mean(spe['random_forest']),\n",
        "                            np.mean(spe['gradient_boosting']), np.mean(spe['logistic_regression']),\n",
        "                            np.mean(spe['bagging']), np.mean(spe['mlp'])]\n",
        "\n",
        "    df.loc['prc_1'] = [np.mean(prec_p['svc_linear']), np.mean(prec_p['svc_rbf']), np.mean(prec_p['random_forest']),\n",
        "                            np.mean(prec_p['gradient_boosting']), np.mean(prec_p['logistic_regression']),\n",
        "                            np.mean(prec_p['bagging']), np.mean(prec_p['mlp'])]\n",
        "\n",
        "    df.loc['prc_0'] = [np.mean(prec_n['svc_linear']), np.mean(prec_n['svc_rbf']), np.mean(prec_n['random_forest']),\n",
        "                            np.mean(prec_n['gradient_boosting']), np.mean(prec_n['logistic_regression']),\n",
        "                            np.mean(prec_n['bagging']), np.mean(prec_n['mlp'])]\n",
        "\n",
        "    # Set caption for DataFrame\n",
        "    df = df.style.set_caption('Performance for external validation')\n",
        "\n",
        "    # Record the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(df)\n",
        "\n",
        "    # Print the total time taken to run external-validation\n",
        "    print(f\"Total time taken to run external-validation: {total_time:.2f} seconds\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf112f5",
      "metadata": {
        "id": "dcf112f5"
      },
      "source": [
        "Cross-validation usando dados treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "19fe7911",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "19fe7911",
        "outputId": "3d3e8896-0373-42bd-f28a-43168b9e8530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79972c7be190>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_6fbdb\" class=\"dataframe\">\n",
              "  <caption>Average performance and standard deviation among 5-fold cross-validation</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_6fbdb_level0_col0\" class=\"col_heading level0 col0\" >svc_linear</th>\n",
              "      <th id=\"T_6fbdb_level0_col1\" class=\"col_heading level0 col1\" >svc_rbf</th>\n",
              "      <th id=\"T_6fbdb_level0_col2\" class=\"col_heading level0 col2\" >random_forest</th>\n",
              "      <th id=\"T_6fbdb_level0_col3\" class=\"col_heading level0 col3\" >gradient_boosting</th>\n",
              "      <th id=\"T_6fbdb_level0_col4\" class=\"col_heading level0 col4\" >logistic_regression</th>\n",
              "      <th id=\"T_6fbdb_level0_col5\" class=\"col_heading level0 col5\" >bagging</th>\n",
              "      <th id=\"T_6fbdb_level0_col6\" class=\"col_heading level0 col6\" >mlp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row0\" class=\"row_heading level0 row0\" >auc (mean)</th>\n",
              "      <td id=\"T_6fbdb_row0_col0\" class=\"data row0 col0\" >0.858678</td>\n",
              "      <td id=\"T_6fbdb_row0_col1\" class=\"data row0 col1\" >0.887198</td>\n",
              "      <td id=\"T_6fbdb_row0_col2\" class=\"data row0 col2\" >0.938910</td>\n",
              "      <td id=\"T_6fbdb_row0_col3\" class=\"data row0 col3\" >0.877670</td>\n",
              "      <td id=\"T_6fbdb_row0_col4\" class=\"data row0 col4\" >0.859167</td>\n",
              "      <td id=\"T_6fbdb_row0_col5\" class=\"data row0 col5\" >0.902740</td>\n",
              "      <td id=\"T_6fbdb_row0_col6\" class=\"data row0 col6\" >0.880429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row1\" class=\"row_heading level0 row1\" >auc (stdev)</th>\n",
              "      <td id=\"T_6fbdb_row1_col0\" class=\"data row1 col0\" >0.012982</td>\n",
              "      <td id=\"T_6fbdb_row1_col1\" class=\"data row1 col1\" >0.012456</td>\n",
              "      <td id=\"T_6fbdb_row1_col2\" class=\"data row1 col2\" >0.009866</td>\n",
              "      <td id=\"T_6fbdb_row1_col3\" class=\"data row1 col3\" >0.010636</td>\n",
              "      <td id=\"T_6fbdb_row1_col4\" class=\"data row1 col4\" >0.011468</td>\n",
              "      <td id=\"T_6fbdb_row1_col5\" class=\"data row1 col5\" >0.012578</td>\n",
              "      <td id=\"T_6fbdb_row1_col6\" class=\"data row1 col6\" >0.017865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row2\" class=\"row_heading level0 row2\" >rcl_1 (mean)</th>\n",
              "      <td id=\"T_6fbdb_row2_col0\" class=\"data row2 col0\" >0.779829</td>\n",
              "      <td id=\"T_6fbdb_row2_col1\" class=\"data row2 col1\" >0.831780</td>\n",
              "      <td id=\"T_6fbdb_row2_col2\" class=\"data row2 col2\" >0.888376</td>\n",
              "      <td id=\"T_6fbdb_row2_col3\" class=\"data row2 col3\" >0.805025</td>\n",
              "      <td id=\"T_6fbdb_row2_col4\" class=\"data row2 col4\" >0.772633</td>\n",
              "      <td id=\"T_6fbdb_row2_col5\" class=\"data row2 col5\" >0.854425</td>\n",
              "      <td id=\"T_6fbdb_row2_col6\" class=\"data row2 col6\" >0.872939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row3\" class=\"row_heading level0 row3\" >rcl_1 (stdev)</th>\n",
              "      <td id=\"T_6fbdb_row3_col0\" class=\"data row3 col0\" >0.017784</td>\n",
              "      <td id=\"T_6fbdb_row3_col1\" class=\"data row3 col1\" >0.017961</td>\n",
              "      <td id=\"T_6fbdb_row3_col2\" class=\"data row3 col2\" >0.013154</td>\n",
              "      <td id=\"T_6fbdb_row3_col3\" class=\"data row3 col3\" >0.023571</td>\n",
              "      <td id=\"T_6fbdb_row3_col4\" class=\"data row3 col4\" >0.015317</td>\n",
              "      <td id=\"T_6fbdb_row3_col5\" class=\"data row3 col5\" >0.034394</td>\n",
              "      <td id=\"T_6fbdb_row3_col6\" class=\"data row3 col6\" >0.024439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row4\" class=\"row_heading level0 row4\" >rcl_0 (mean)</th>\n",
              "      <td id=\"T_6fbdb_row4_col0\" class=\"data row4 col0\" >0.786008</td>\n",
              "      <td id=\"T_6fbdb_row4_col1\" class=\"data row4 col1\" >0.793218</td>\n",
              "      <td id=\"T_6fbdb_row4_col2\" class=\"data row4 col2\" >0.816356</td>\n",
              "      <td id=\"T_6fbdb_row4_col3\" class=\"data row4 col3\" >0.785484</td>\n",
              "      <td id=\"T_6fbdb_row4_col4\" class=\"data row4 col4\" >0.779317</td>\n",
              "      <td id=\"T_6fbdb_row4_col5\" class=\"data row4 col5\" >0.806576</td>\n",
              "      <td id=\"T_6fbdb_row4_col6\" class=\"data row4 col6\" >0.776755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row5\" class=\"row_heading level0 row5\" >rcl_0 (stdev)</th>\n",
              "      <td id=\"T_6fbdb_row5_col0\" class=\"data row5 col0\" >0.018132</td>\n",
              "      <td id=\"T_6fbdb_row5_col1\" class=\"data row5 col1\" >0.014668</td>\n",
              "      <td id=\"T_6fbdb_row5_col2\" class=\"data row5 col2\" >0.011601</td>\n",
              "      <td id=\"T_6fbdb_row5_col3\" class=\"data row5 col3\" >0.011770</td>\n",
              "      <td id=\"T_6fbdb_row5_col4\" class=\"data row5 col4\" >0.016593</td>\n",
              "      <td id=\"T_6fbdb_row5_col5\" class=\"data row5 col5\" >0.008850</td>\n",
              "      <td id=\"T_6fbdb_row5_col6\" class=\"data row5 col6\" >0.013048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row6\" class=\"row_heading level0 row6\" >prc_1 (mean)</th>\n",
              "      <td id=\"T_6fbdb_row6_col0\" class=\"data row6 col0\" >0.784740</td>\n",
              "      <td id=\"T_6fbdb_row6_col1\" class=\"data row6 col1\" >0.800858</td>\n",
              "      <td id=\"T_6fbdb_row6_col2\" class=\"data row6 col2\" >0.828737</td>\n",
              "      <td id=\"T_6fbdb_row6_col3\" class=\"data row6 col3\" >0.789635</td>\n",
              "      <td id=\"T_6fbdb_row6_col4\" class=\"data row6 col4\" >0.777902</td>\n",
              "      <td id=\"T_6fbdb_row6_col5\" class=\"data row6 col5\" >0.815292</td>\n",
              "      <td id=\"T_6fbdb_row6_col6\" class=\"data row6 col6\" >0.796239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row7\" class=\"row_heading level0 row7\" >prc_1 (stdev)</th>\n",
              "      <td id=\"T_6fbdb_row7_col0\" class=\"data row7 col0\" >0.016958</td>\n",
              "      <td id=\"T_6fbdb_row7_col1\" class=\"data row7 col1\" >0.014555</td>\n",
              "      <td id=\"T_6fbdb_row7_col2\" class=\"data row7 col2\" >0.009987</td>\n",
              "      <td id=\"T_6fbdb_row7_col3\" class=\"data row7 col3\" >0.008795</td>\n",
              "      <td id=\"T_6fbdb_row7_col4\" class=\"data row7 col4\" >0.015251</td>\n",
              "      <td id=\"T_6fbdb_row7_col5\" class=\"data row7 col5\" >0.008530</td>\n",
              "      <td id=\"T_6fbdb_row7_col6\" class=\"data row7 col6\" >0.013191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row8\" class=\"row_heading level0 row8\" >prc_0 (mean)</th>\n",
              "      <td id=\"T_6fbdb_row8_col0\" class=\"data row8 col0\" >0.781228</td>\n",
              "      <td id=\"T_6fbdb_row8_col1\" class=\"data row8 col1\" >0.825131</td>\n",
              "      <td id=\"T_6fbdb_row8_col2\" class=\"data row8 col2\" >0.879811</td>\n",
              "      <td id=\"T_6fbdb_row8_col3\" class=\"data row8 col3\" >0.801620</td>\n",
              "      <td id=\"T_6fbdb_row8_col4\" class=\"data row8 col4\" >0.774163</td>\n",
              "      <td id=\"T_6fbdb_row8_col5\" class=\"data row8 col5\" >0.848241</td>\n",
              "      <td id=\"T_6fbdb_row8_col6\" class=\"data row8 col6\" >0.859820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6fbdb_level0_row9\" class=\"row_heading level0 row9\" >prc_0 (stdev)</th>\n",
              "      <td id=\"T_6fbdb_row9_col0\" class=\"data row9 col0\" >0.016936</td>\n",
              "      <td id=\"T_6fbdb_row9_col1\" class=\"data row9 col1\" >0.017777</td>\n",
              "      <td id=\"T_6fbdb_row9_col2\" class=\"data row9 col2\" >0.013432</td>\n",
              "      <td id=\"T_6fbdb_row9_col3\" class=\"data row9 col3\" >0.018798</td>\n",
              "      <td id=\"T_6fbdb_row9_col4\" class=\"data row9 col4\" >0.014719</td>\n",
              "      <td id=\"T_6fbdb_row9_col5\" class=\"data row9 col5\" >0.031066</td>\n",
              "      <td id=\"T_6fbdb_row9_col6\" class=\"data row9 col6\" >0.025311</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken to run cross-validation: 98.07 seconds\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/arturrur/mc853/refs/heads/main/data/training/treino.csv')\n",
        "\n",
        "# impute NaN values in Escolaridade column\n",
        "df_full = impute_missing(df)\n",
        "\n",
        "# resample data so death and survival are 50-50\n",
        "target_feature = df.columns[-1]\n",
        "x_balanced, y_balanced = data_sample(df_full.drop(columns=[target_feature]), df_full[target_feature])\n",
        "df_balanced = pd.concat([x_balanced, y_balanced], axis=1)\n",
        "\n",
        "# evaluates\n",
        "evaluate_cv(df_balanced);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validação externa do modelo"
      ],
      "metadata": {
        "id": "_02wiVM02Gfl"
      },
      "id": "_02wiVM02Gfl"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c7b6121c-30a7-4308-a612-031828306cc5",
      "metadata": {
        "id": "c7b6121c-30a7-4308-a612-031828306cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "cb5d1052-592a-4ec1-9ee6-d02cf91978a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79972c1a4d10>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_1c117\" class=\"dataframe\">\n",
              "  <caption>Performance for external validation</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_1c117_level0_col0\" class=\"col_heading level0 col0\" >svc_linear</th>\n",
              "      <th id=\"T_1c117_level0_col1\" class=\"col_heading level0 col1\" >svc_rbf</th>\n",
              "      <th id=\"T_1c117_level0_col2\" class=\"col_heading level0 col2\" >random_forest</th>\n",
              "      <th id=\"T_1c117_level0_col3\" class=\"col_heading level0 col3\" >gradient_boosting</th>\n",
              "      <th id=\"T_1c117_level0_col4\" class=\"col_heading level0 col4\" >logistic_regression</th>\n",
              "      <th id=\"T_1c117_level0_col5\" class=\"col_heading level0 col5\" >bagging</th>\n",
              "      <th id=\"T_1c117_level0_col6\" class=\"col_heading level0 col6\" >mlp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_1c117_level0_row0\" class=\"row_heading level0 row0\" >auc</th>\n",
              "      <td id=\"T_1c117_row0_col0\" class=\"data row0 col0\" >0.787177</td>\n",
              "      <td id=\"T_1c117_row0_col1\" class=\"data row0 col1\" >0.769698</td>\n",
              "      <td id=\"T_1c117_row0_col2\" class=\"data row0 col2\" >0.769282</td>\n",
              "      <td id=\"T_1c117_row0_col3\" class=\"data row0 col3\" >0.783426</td>\n",
              "      <td id=\"T_1c117_row0_col4\" class=\"data row0 col4\" >0.788153</td>\n",
              "      <td id=\"T_1c117_row0_col5\" class=\"data row0 col5\" >0.738430</td>\n",
              "      <td id=\"T_1c117_row0_col6\" class=\"data row0 col6\" >0.741093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c117_level0_row1\" class=\"row_heading level0 row1\" >rcl_1</th>\n",
              "      <td id=\"T_1c117_row1_col0\" class=\"data row1 col0\" >0.770883</td>\n",
              "      <td id=\"T_1c117_row1_col1\" class=\"data row1 col1\" >0.715990</td>\n",
              "      <td id=\"T_1c117_row1_col2\" class=\"data row1 col2\" >0.665871</td>\n",
              "      <td id=\"T_1c117_row1_col3\" class=\"data row1 col3\" >0.754177</td>\n",
              "      <td id=\"T_1c117_row1_col4\" class=\"data row1 col4\" >0.766110</td>\n",
              "      <td id=\"T_1c117_row1_col5\" class=\"data row1 col5\" >0.606205</td>\n",
              "      <td id=\"T_1c117_row1_col6\" class=\"data row1 col6\" >0.608592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c117_level0_row2\" class=\"row_heading level0 row2\" >rcl_0</th>\n",
              "      <td id=\"T_1c117_row2_col0\" class=\"data row2 col0\" >0.670644</td>\n",
              "      <td id=\"T_1c117_row2_col1\" class=\"data row2 col1\" >0.713604</td>\n",
              "      <td id=\"T_1c117_row2_col2\" class=\"data row2 col2\" >0.754177</td>\n",
              "      <td id=\"T_1c117_row2_col3\" class=\"data row2 col3\" >0.692124</td>\n",
              "      <td id=\"T_1c117_row2_col4\" class=\"data row2 col4\" >0.680191</td>\n",
              "      <td id=\"T_1c117_row2_col5\" class=\"data row2 col5\" >0.758950</td>\n",
              "      <td id=\"T_1c117_row2_col6\" class=\"data row2 col6\" >0.730310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c117_level0_row3\" class=\"row_heading level0 row3\" >prc_1</th>\n",
              "      <td id=\"T_1c117_row3_col0\" class=\"data row3 col0\" >0.700651</td>\n",
              "      <td id=\"T_1c117_row3_col1\" class=\"data row3 col1\" >0.714286</td>\n",
              "      <td id=\"T_1c117_row3_col2\" class=\"data row3 col2\" >0.730366</td>\n",
              "      <td id=\"T_1c117_row3_col3\" class=\"data row3 col3\" >0.710112</td>\n",
              "      <td id=\"T_1c117_row3_col4\" class=\"data row3 col4\" >0.705495</td>\n",
              "      <td id=\"T_1c117_row3_col5\" class=\"data row3 col5\" >0.715493</td>\n",
              "      <td id=\"T_1c117_row3_col6\" class=\"data row3 col6\" >0.692935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1c117_level0_row4\" class=\"row_heading level0 row4\" >prc_0</th>\n",
              "      <td id=\"T_1c117_row4_col0\" class=\"data row4 col0\" >0.745358</td>\n",
              "      <td id=\"T_1c117_row4_col1\" class=\"data row4 col1\" >0.715311</td>\n",
              "      <td id=\"T_1c117_row4_col2\" class=\"data row4 col2\" >0.692982</td>\n",
              "      <td id=\"T_1c117_row4_col3\" class=\"data row4 col3\" >0.737913</td>\n",
              "      <td id=\"T_1c117_row4_col4\" class=\"data row4 col4\" >0.744125</td>\n",
              "      <td id=\"T_1c117_row4_col5\" class=\"data row4 col5\" >0.658385</td>\n",
              "      <td id=\"T_1c117_row4_col6\" class=\"data row4 col6\" >0.651064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken to run external-validation: 26.00 seconds\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_csv('https://raw.githubusercontent.com/arturrur/mc853/refs/heads/main/data/test/teste.csv')\n",
        "\n",
        "# impute missing values for df_test\n",
        "df_full_test = impute_missing(df_test)\n",
        "\n",
        "# resample test data\n",
        "target_feature_test = df.columns[-1]\n",
        "x_balanced_test, y_balanced_test = data_sample(df_full_test.drop(columns=[target_feature_test]), df_full_test[target_feature_test])\n",
        "df_balanced_test = pd.concat([x_balanced_test, y_balanced_test], axis=1)\n",
        "\n",
        "# evaluates\n",
        "evaluate_external(df_balanced, df_balanced_test);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}