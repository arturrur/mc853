{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arturrur/mc853/blob/using-KNN-imputer/fun%C3%A7%C3%B5es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a28f6cca-9c5e-4228-9248-dc682e75a670",
      "metadata": {
        "id": "a28f6cca-9c5e-4228-9248-dc682e75a670"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import pandas as pd                       # For data manipulation and analysis\n",
        "import numpy as np                        # For numerical computing\n",
        "import time                               # For tracking time\n",
        "import math                               # For mathematical operations\n",
        "import warnings                           # For managing warnings\n",
        "\n",
        "import shap                               # For SHAP (SHapley Additive exPlanations) values\n",
        "\n",
        "import imblearn                           # For dealing with imbalanced datasets\n",
        "from imblearn.over_sampling import RandomOverSampler   # For oversampling\n",
        "from imblearn.under_sampling import RandomUnderSampler # For undersampling\n",
        "\n",
        "\n",
        "import seaborn as sns                     # For statistical data visualization\n",
        "import matplotlib.pyplot as plt           # For creating visualizations\n",
        "import matplotlib.patches as mpatches     # For drawing patches in plots\n",
        "import matplotlib.colors as mcolors       # For defining custom colors in plots\n",
        "import matplotlib.ticker as ticker        # For formatting tick marks on plots\n",
        "from matplotlib.ticker import FuncFormatter         # For custom tick formatting\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler      # For feature scaling\n",
        "from sklearn.model_selection import (StratifiedKFold,\n",
        "                                     GridSearchCV) # For splitting data into train and test sets\n",
        "\n",
        "from sklearn.metrics import (roc_auc_score,           # For evaluating model performance\n",
        "                             recall_score,\n",
        "                             make_scorer,\n",
        "                             balanced_accuracy_score,\n",
        "                             confusion_matrix,\n",
        "                             precision_recall_curve,\n",
        "                             f1_score)\n",
        "\n",
        "from sklearn.svm import SVC                           # For Support Vector Classifier\n",
        "from sklearn.linear_model import LogisticRegression   # For Logistic Regression Classifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, # For ensemble classifiers\n",
        "                              GradientBoostingClassifier,\n",
        "                              BaggingClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier # For KNN\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier      # For Multi-layer Perceptron Classifier\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "import re\n",
        "from sklearn.calibration import CalibratedClassifierCV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "67ebf0c6-b108-4e25-9a3a-2da7f2717f6a",
      "metadata": {
        "id": "67ebf0c6-b108-4e25-9a3a-2da7f2717f6a"
      },
      "outputs": [],
      "source": [
        "# Set the number of folds for cross-validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
        "\n",
        "# Set the number of kfolds for grid search\n",
        "gskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17)\n",
        "\n",
        "# Choice of the best hyperparameters through balanced accuracy metric\n",
        "perf = balanced_accuracy_score\n",
        "\n",
        "# Set preprocessing: StandardScaler for feature standardization\n",
        "preprocessing = StandardScaler()\n",
        "\n",
        "# Initialize KNNImputer with the specified number of neighbors\n",
        "imputer = KNNImputer(n_neighbors=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ff5a01e1-3ee8-4e70-b1ed-71d1795095ae",
      "metadata": {
        "id": "ff5a01e1-3ee8-4e70-b1ed-71d1795095ae"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary containing various classification algorithms and their parameters used in grid search\n",
        "\n",
        "algorithms = {\n",
        "    'knn': (KNeighborsClassifier(), {\"n_neighbors\": [1, 5, 10], \"weights\": ('uniform', 'distance')}),\n",
        "    # K-Nearest Neighbors Classifier\n",
        "\n",
        "    'random_forest': (RandomForestClassifier(random_state=0, class_weight='balanced'), {\n",
        "        'n_estimators': [10, 50, 100, 200],\n",
        "        'min_samples_leaf': [5, 10, 20],\n",
        "        'max_depth': [10, 25, 50],\n",
        "        'min_samples_split': [5, 10, 30],\n",
        "        'max_features': ('sqrt', 'log2'),\n",
        "        }),\n",
        "    # Random Forest Classifier\n",
        "\n",
        "    'logistic_regression': (LogisticRegression(), {'class_weight': ('balanced', {0:1, 1:2}, {0:1, 1:3})}),\n",
        "    # Logistic Regression Classifier\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zA7am_xbEmFE"
      },
      "id": "zA7am_xbEmFE"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1a580b6-211d-4a4d-85a0-e62afb275421",
      "metadata": {
        "id": "a1a580b6-211d-4a4d-85a0-e62afb275421"
      },
      "outputs": [],
      "source": [
        "def data_sample(X, y):\n",
        "    '''\n",
        "    Receives a set of features and target feature separately.\n",
        "    Returns balanced data, with the same number of samples in both classes.\n",
        "    If the minority class is less than 5%, applies oversampling and undersampling.\n",
        "    Otherwise, applies only undersampling.\n",
        "    Parameters:\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The feature matrix.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The target variable.\n",
        "\n",
        "    Returns:\n",
        "        X_resampled : array-like, shape (n_samples_resampled, n_features)\n",
        "            The resampled feature matrix.\n",
        "        y_resampled : array-like, shape (n_samples_resampled,)\n",
        "            The resampled target variable.\n",
        "    '''\n",
        "\n",
        "    # Define sampling strategies\n",
        "    undersample = RandomUnderSampler(sampling_strategy='majority', random_state=1)\n",
        "    oversample = RandomOverSampler(sampling_strategy=0.2, random_state=1)\n",
        "\n",
        "    # Identify the minority class\n",
        "    count_1 = (y == 1).sum()\n",
        "    count_0 = (y == 0).sum()\n",
        "    count_min = min(count_0, count_1)\n",
        "    count_max = max(count_0, count_1)\n",
        "\n",
        "\n",
        "    # Calculate the percentage of the minority class compared to the total number of instances\n",
        "    ratio = (count_min / count_max)\n",
        "\n",
        "    # If the minority class is more than 60% of the majority class, do not apply any resampling technique\n",
        "    if ratio > 0.6:\n",
        "        X_resampled, y_resampled = X, y\n",
        "\n",
        "    # Check if the percentage of class 1 is at least 5% of the total number of instances\n",
        "    # If it is less than 5%, apply both over and under sampling\n",
        "    else:\n",
        "        if ratio <= 0.2:\n",
        "            X_resampled, y_resampled = oversample.fit_resample(X, y)\n",
        "            X_resampled, y_resampled = undersample.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "        # Otherwise, apply only undersampling\n",
        "        else:\n",
        "            X_resampled, y_resampled = undersample.fit_resample(X, y)\n",
        "\n",
        "    return X_resampled, y_resampled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7192dcb9-be8b-41ed-ac10-9302fad1a7a6",
      "metadata": {
        "id": "7192dcb9-be8b-41ed-ac10-9302fad1a7a6"
      },
      "outputs": [],
      "source": [
        "def impute_missing(data, n_neighbors=3):\n",
        "    \"\"\"\n",
        "    Impute missing values using the K-nearest neighbors algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        data (pd.DataFrame): Input DataFrame with missing values.\n",
        "        n_neighbors (int, optional): Number of neighbors to use for imputation. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with missing values imputed using KNN.\n",
        "    \"\"\"\n",
        "    # Initialize KNNImputer with the specified number of neighbors\n",
        "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "\n",
        "    # Perform imputation\n",
        "    imputed_data = imputer.fit_transform(data)\n",
        "\n",
        "    # Convert the imputed array back to a DataFrame\n",
        "    imputed_df = pd.DataFrame(imputed_data, columns=data.columns, index=data.index)\n",
        "\n",
        "    return imputed_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bfe8f23f-9dd9-4e7f-9844-7dd573137f53",
      "metadata": {
        "id": "bfe8f23f-9dd9-4e7f-9844-7dd573137f53"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "def evaluate_cv(data):\n",
        "    '''\n",
        "    Receives data to be evaluated and returns the average performance inside cross-validation, using 3 metrics.\n",
        "    Applies over-under sampling to get balanced datasets and standardizes features.\n",
        "\n",
        "    Parameters:\n",
        "    data : DataFrame\n",
        "        The dataset containing features and the target variable.\n",
        "\n",
        "    Returns:\n",
        "    df : DataFrame\n",
        "        A DataFrame containing the mean and standard deviation of each algorithm's performance across 5-fold cross-validation.\n",
        "        The performance metrics include AUC (mean and standard deviation), sensitivity (mean and standard deviation),\n",
        "        specificity (mean and standard deviation), prec_n (mean and standard deviation), and prec_p (mean and standard deviation).\n",
        "    '''\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Identify the target column\n",
        "    target_feature = data.columns[-1]\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = data.drop(columns=[target_feature])\n",
        "    y = data[target_feature]\n",
        "\n",
        "    # Initialize dictionaries to store metrics for each algorithm\n",
        "    sen = {}\n",
        "    spe = {}\n",
        "    auc = {}\n",
        "    prec_n = {}  # Negative precision\n",
        "    prec_p = {}  # Positive precision\n",
        "\n",
        "    for algorithm in algorithms.keys():\n",
        "        sen[algorithm] = []\n",
        "        spe[algorithm] = []\n",
        "        auc[algorithm] = []\n",
        "        prec_n[algorithm] = []\n",
        "        prec_p[algorithm] = []\n",
        "\n",
        "    for algorithm, (clf, parameters) in algorithms.items():\n",
        "      best = GridSearchCV(clf, parameters, cv=gskf, scoring=(make_scorer(perf)))\n",
        "\n",
        "      # Iterate through each round of the cross-validation\n",
        "\n",
        "      for train, test in kf.split(X, y):\n",
        "        # Allocate train and test data\n",
        "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "        y_train, y_test = y.iloc[train], y.iloc[test]\n",
        "\n",
        "        # # Apply over-under sampling\n",
        "        # X_train, y_train = data_sample(X_train, y_train)\n",
        "\n",
        "        # X_train = imputer.fit_transform(X_train)\n",
        "        # X_test = imputer.transform(X_test)\n",
        "\n",
        "        # Standardize features\n",
        "        X_train = preprocessing.fit_transform(X_train)\n",
        "        X_test = preprocessing.transform(X_test)\n",
        "\n",
        "        # Find best hyperparameters\n",
        "        best.fit((X_train), y_train)\n",
        "\n",
        "        # Make predictions for the test data\n",
        "        y_pred = best.predict(X_test)\n",
        "\n",
        "        # Calculate sensitivity and specificity\n",
        "        recallscore = recall_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "        sen[algorithm].append(recallscore[1])\n",
        "        spe[algorithm].append(recallscore[0])\n",
        "\n",
        "        # Calculate precision for each class\n",
        "        prec_score = precision_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "        prec_n[algorithm].append(prec_score[0])\n",
        "        prec_p[algorithm].append(prec_score[1])\n",
        "\n",
        "        # Calculate the area under the ROC curve\n",
        "        aucscore = roc_auc_score(y_test, (best.predict_proba((X_test)))[:, 1])\n",
        "        auc[algorithm].append(aucscore)\n",
        "\n",
        "    # Create a DataFrame with the mean and standard deviation of each algorithm's performance across 5 folds\n",
        "    df = pd.DataFrame(columns=list(algorithms.keys()))\n",
        "\n",
        "    df.loc['auc (mean)'] = [np.mean(auc['knn']), np.mean(auc['random_forest']), np.mean(auc['logistic_regression'])]\n",
        "    df.loc['auc (stdev)'] = [np.std(auc['knn']), np.std(auc['random_forest']), np.std(auc['logistic_regression'])]\n",
        "\n",
        "    df.loc['rcl_1 (mean)'] = [np.mean(sen['knn']), np.mean(sen['random_forest']), np.mean(sen['logistic_regression'])]\n",
        "    df.loc['rcl_1 (stdev)'] = [np.std(sen['knn']), np.std(sen['random_forest']), np.std(sen['logistic_regression'])]\n",
        "\n",
        "    df.loc['rcl_0 (mean)'] = [np.mean(spe['knn']), np.mean(spe['random_forest']), np.mean(spe['logistic_regression'])]\n",
        "    df.loc['rcl_0 (stdev)'] = [np.std(spe['knn']), np.std(spe['random_forest']), np.std(spe['logistic_regression'])]\n",
        "\n",
        "    df.loc['prc_1 (mean)'] = [np.mean(prec_p['knn']), np.mean(prec_p['random_forest']), np.mean(prec_p['logistic_regression'])]\n",
        "    df.loc['prc_1 (stdev)'] = [np.std(prec_p['knn']), np.std(prec_p['random_forest']), np.std(prec_p['logistic_regression'])]\n",
        "\n",
        "    df.loc['prc_0 (mean)'] = [np.mean(prec_n['knn']), np.mean(prec_n['random_forest']), np.mean(prec_n['logistic_regression'])]\n",
        "    df.loc['prc_0 (stdev)'] = [np.std(prec_n['knn']), np.std(prec_n['random_forest']), np.std(prec_n['logistic_regression'])]\n",
        "\n",
        "\n",
        "    # Set caption for DataFrame\n",
        "    df = df.style.set_caption('Average performance and standard deviation among 5-fold cross-validation')\n",
        "\n",
        "    # Record the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(df)\n",
        "\n",
        "    # Print the total time taken to run cross-validation\n",
        "    print(f\"Total time taken to run cross-validation: {total_time:.2f} seconds\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e691d3fe-206c-4b5b-ad01-c57388737cf3",
      "metadata": {
        "id": "e691d3fe-206c-4b5b-ad01-c57388737cf3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "def evaluate_external(data, data_test):\n",
        "    '''\n",
        "    Receives data and data_test to be evaluated and returns the average performance, using 3 metrics.\n",
        "    Applies over-under sampling to get balanced datasets and standardizes features.\n",
        "\n",
        "    Parameters:\n",
        "    data : DataFrame\n",
        "        The training dataset containing features and the target variable.\n",
        "    data_test : DataFrame\n",
        "        The test dataset containing features and the target variable.\n",
        "\n",
        "    Returns:\n",
        "    df : DataFrame\n",
        "        A DataFrame containing the mean performance of each algorithm across external validation.\n",
        "        The performance metrics include AUC (mean), sensitivity (mean), specificity (mean), prec_n (mean), and prec_p (mean).\n",
        "    '''\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Identify the target column\n",
        "    target_feature = data.columns[-1]\n",
        "\n",
        "    # Separate features (X) and target (y) for training data\n",
        "    X = data.drop(columns=[target_feature])\n",
        "    y = data[target_feature]\n",
        "\n",
        "    # Separate features (X) and target (y) for test data\n",
        "    X_test = data_test.drop(columns=[target_feature])\n",
        "    y_test = data_test[target_feature]\n",
        "\n",
        "    # Initialize dictionaries to store metrics for each algorithm\n",
        "    sen = {}\n",
        "    spe = {}\n",
        "    auc = {}\n",
        "    prec_n = {}  # Negative precision\n",
        "    prec_p = {}  # Positive precision\n",
        "\n",
        "    for algorithm in algorithms.keys():\n",
        "        sen[algorithm] = []\n",
        "        spe[algorithm] = []\n",
        "        auc[algorithm] = []\n",
        "        prec_n[algorithm] = []\n",
        "        prec_p[algorithm] = []\n",
        "\n",
        "    # Apply over-under sampling to training data\n",
        "    #X_train, y_train = data_sample(X, y)\n",
        "\n",
        "\n",
        "    #X_train = imputer.fit_transform(X_train)\n",
        "    #X_test = imputer.transform(X_test)\n",
        "    X_train = X\n",
        "    y_train = y\n",
        "\n",
        "    # Apply preprocessing to both training and test data\n",
        "    X_train = preprocessing.fit_transform(X_train)\n",
        "    X_test = preprocessing.transform(X_test)\n",
        "\n",
        "    # For each algorithm\n",
        "    for algorithm, (clf, parameters) in algorithms.items():\n",
        "        # Train model\n",
        "        best = GridSearchCV(clf, parameters, cv=gskf, scoring=(make_scorer(perf)))\n",
        "        best.fit((X_train), y_train)\n",
        "\n",
        "        print(\"Best hyperparameters found:\", best.best_params_)\n",
        "        y_pred = best.predict(X_test)\n",
        "\n",
        "\n",
        "        # METRICS\n",
        "        # Calculate sensitivity and specificity\n",
        "        recallscore = recall_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "        sen[algorithm].append(recallscore[1])\n",
        "        spe[algorithm].append(recallscore[0])\n",
        "\n",
        "        # Calculate precision for each class\n",
        "        prec_score = precision_score(y_test, y_pred, labels=[0, 1], average=None)\n",
        "        prec_n[algorithm].append(prec_score[0])\n",
        "        prec_p[algorithm].append(prec_score[1])\n",
        "\n",
        "        # Calculate the area under the ROC curve\n",
        "        aucscore = roc_auc_score(y_test, (best.predict_proba((X_test)))[:, 1])\n",
        "        auc[algorithm].append(aucscore)\n",
        "\n",
        "        print(f\"confusion matrix for algorithm: {algorithm}\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Create a DataFrame with the mean performance of each algorithm across the external validation\n",
        "    df = pd.DataFrame(columns=list(algorithms.keys()))\n",
        "\n",
        "    df.loc['auc (mean)'] = [np.mean(auc['knn']), np.mean(auc['random_forest']), np.mean(auc['logistic_regression'])]\n",
        "\n",
        "    df.loc['rcl_1 (mean)'] = [np.mean(sen['knn']), np.mean(sen['random_forest']), np.mean(sen['logistic_regression'])]\n",
        "\n",
        "    df.loc['rcl_0 (mean)'] = [np.mean(spe['knn']), np.mean(spe['random_forest']), np.mean(spe['logistic_regression'])]\n",
        "\n",
        "    df.loc['prc_1 (mean)'] = [np.mean(prec_p['knn']), np.mean(prec_p['random_forest']), np.mean(prec_p['logistic_regression'])]\n",
        "\n",
        "    df.loc['prc_0 (mean)'] = [np.mean(prec_n['knn']), np.mean(prec_n['random_forest']), np.mean(prec_n['logistic_regression'])]\n",
        "\n",
        "    # Set caption for DataFrame\n",
        "    df = df.style.set_caption('Performance for external validation')\n",
        "\n",
        "    # Record the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the time taken\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(df)\n",
        "\n",
        "    # Print the total time taken to run external-validation\n",
        "    print(f\"Total time taken to run external-validation: {total_time:.2f} seconds\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf112f5",
      "metadata": {
        "id": "dcf112f5"
      },
      "source": [
        "Cross-validation usando dados treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "19fe7911",
      "metadata": {
        "id": "19fe7911",
        "outputId": "47fde68c-7551-4845-f494-534e4f78dba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x78e06ab82510>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_ea820\" class=\"dataframe\">\n",
              "  <caption>Average performance and standard deviation among 5-fold cross-validation</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ea820_level0_col0\" class=\"col_heading level0 col0\" >knn</th>\n",
              "      <th id=\"T_ea820_level0_col1\" class=\"col_heading level0 col1\" >random_forest</th>\n",
              "      <th id=\"T_ea820_level0_col2\" class=\"col_heading level0 col2\" >logistic_regression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row0\" class=\"row_heading level0 row0\" >auc (mean)</th>\n",
              "      <td id=\"T_ea820_row0_col0\" class=\"data row0 col0\" >0.922625</td>\n",
              "      <td id=\"T_ea820_row0_col1\" class=\"data row0 col1\" >0.900336</td>\n",
              "      <td id=\"T_ea820_row0_col2\" class=\"data row0 col2\" >0.859171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row1\" class=\"row_heading level0 row1\" >auc (stdev)</th>\n",
              "      <td id=\"T_ea820_row1_col0\" class=\"data row1 col0\" >0.015536</td>\n",
              "      <td id=\"T_ea820_row1_col1\" class=\"data row1 col1\" >0.010533</td>\n",
              "      <td id=\"T_ea820_row1_col2\" class=\"data row1 col2\" >0.011468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row2\" class=\"row_heading level0 row2\" >rcl_1 (mean)</th>\n",
              "      <td id=\"T_ea820_row2_col0\" class=\"data row2 col0\" >0.872426</td>\n",
              "      <td id=\"T_ea820_row2_col1\" class=\"data row2 col1\" >0.847207</td>\n",
              "      <td id=\"T_ea820_row2_col2\" class=\"data row2 col2\" >0.772633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row3\" class=\"row_heading level0 row3\" >rcl_1 (stdev)</th>\n",
              "      <td id=\"T_ea820_row3_col0\" class=\"data row3 col0\" >0.021434</td>\n",
              "      <td id=\"T_ea820_row3_col1\" class=\"data row3 col1\" >0.016559</td>\n",
              "      <td id=\"T_ea820_row3_col2\" class=\"data row3 col2\" >0.015317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row4\" class=\"row_heading level0 row4\" >rcl_0 (mean)</th>\n",
              "      <td id=\"T_ea820_row4_col0\" class=\"data row4 col0\" >0.789097</td>\n",
              "      <td id=\"T_ea820_row4_col1\" class=\"data row4 col1\" >0.787038</td>\n",
              "      <td id=\"T_ea820_row4_col2\" class=\"data row4 col2\" >0.779317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row5\" class=\"row_heading level0 row5\" >rcl_0 (stdev)</th>\n",
              "      <td id=\"T_ea820_row5_col0\" class=\"data row5 col0\" >0.006213</td>\n",
              "      <td id=\"T_ea820_row5_col1\" class=\"data row5 col1\" >0.015700</td>\n",
              "      <td id=\"T_ea820_row5_col2\" class=\"data row5 col2\" >0.016593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row6\" class=\"row_heading level0 row6\" >prc_1 (mean)</th>\n",
              "      <td id=\"T_ea820_row6_col0\" class=\"data row6 col0\" >0.805210</td>\n",
              "      <td id=\"T_ea820_row6_col1\" class=\"data row6 col1\" >0.799270</td>\n",
              "      <td id=\"T_ea820_row6_col2\" class=\"data row6 col2\" >0.777902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row7\" class=\"row_heading level0 row7\" >prc_1 (stdev)</th>\n",
              "      <td id=\"T_ea820_row7_col0\" class=\"data row7 col0\" >0.008194</td>\n",
              "      <td id=\"T_ea820_row7_col1\" class=\"data row7 col1\" >0.011481</td>\n",
              "      <td id=\"T_ea820_row7_col2\" class=\"data row7 col2\" >0.015251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row8\" class=\"row_heading level0 row8\" >prc_0 (mean)</th>\n",
              "      <td id=\"T_ea820_row8_col0\" class=\"data row8 col0\" >0.861200</td>\n",
              "      <td id=\"T_ea820_row8_col1\" class=\"data row8 col1\" >0.837673</td>\n",
              "      <td id=\"T_ea820_row8_col2\" class=\"data row8 col2\" >0.774163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea820_level0_row9\" class=\"row_heading level0 row9\" >prc_0 (stdev)</th>\n",
              "      <td id=\"T_ea820_row9_col0\" class=\"data row9 col0\" >0.021214</td>\n",
              "      <td id=\"T_ea820_row9_col1\" class=\"data row9 col1\" >0.014078</td>\n",
              "      <td id=\"T_ea820_row9_col2\" class=\"data row9 col2\" >0.014719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken to run cross-validation: 925.39 seconds\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/arturrur/mc853/refs/heads/using-KNN-imputer/data/training/treino.csv')\n",
        "\n",
        "# impute NaN values in Escolaridade column\n",
        "df_full = impute_missing(df)\n",
        "\n",
        "# resample data so death and survival are 50-50\n",
        "target_feature = df.columns[-1]\n",
        "x_balanced, y_balanced = data_sample(df_full.drop(columns=[target_feature]), df_full[target_feature])\n",
        "df_balanced = pd.concat([x_balanced, y_balanced], axis=1)\n",
        "\n",
        "# evaluates\n",
        "evaluate_cv(df_balanced);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validação externa do modelo"
      ],
      "metadata": {
        "id": "_02wiVM02Gfl"
      },
      "id": "_02wiVM02Gfl"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c7b6121c-30a7-4308-a612-031828306cc5",
      "metadata": {
        "id": "c7b6121c-30a7-4308-a612-031828306cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "1fc879d0-cfbd-496f-abba-41b917812f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters found: {'n_neighbors': 10, 'weights': 'distance'}\n",
            "confusion matrix for algorithm: knn\n",
            "[[4816 1335]\n",
            " [ 226  477]]\n",
            "Best hyperparameters found: {'max_depth': 25, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "confusion matrix for algorithm: random_forest\n",
            "[[4874 1277]\n",
            " [ 181  522]]\n",
            "Best hyperparameters found: {'class_weight': 'balanced'}\n",
            "confusion matrix for algorithm: logistic_regression\n",
            "[[4871 1280]\n",
            " [ 190  513]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x78e0c5f408d0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_30270\" class=\"dataframe\">\n",
              "  <caption>Performance for external validation</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_30270_level0_col0\" class=\"col_heading level0 col0\" >knn</th>\n",
              "      <th id=\"T_30270_level0_col1\" class=\"col_heading level0 col1\" >random_forest</th>\n",
              "      <th id=\"T_30270_level0_col2\" class=\"col_heading level0 col2\" >logistic_regression</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_30270_level0_row0\" class=\"row_heading level0 row0\" >auc (mean)</th>\n",
              "      <td id=\"T_30270_row0_col0\" class=\"data row0 col0\" >0.799790</td>\n",
              "      <td id=\"T_30270_row0_col1\" class=\"data row0 col1\" >0.852138</td>\n",
              "      <td id=\"T_30270_row0_col2\" class=\"data row0 col2\" >0.852861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_30270_level0_row1\" class=\"row_heading level0 row1\" >rcl_1 (mean)</th>\n",
              "      <td id=\"T_30270_row1_col0\" class=\"data row1 col0\" >0.678521</td>\n",
              "      <td id=\"T_30270_row1_col1\" class=\"data row1 col1\" >0.742532</td>\n",
              "      <td id=\"T_30270_row1_col2\" class=\"data row1 col2\" >0.729730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_30270_level0_row2\" class=\"row_heading level0 row2\" >rcl_0 (mean)</th>\n",
              "      <td id=\"T_30270_row2_col0\" class=\"data row2 col0\" >0.782962</td>\n",
              "      <td id=\"T_30270_row2_col1\" class=\"data row2 col1\" >0.792391</td>\n",
              "      <td id=\"T_30270_row2_col2\" class=\"data row2 col2\" >0.791904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_30270_level0_row3\" class=\"row_heading level0 row3\" >prc_1 (mean)</th>\n",
              "      <td id=\"T_30270_row3_col0\" class=\"data row3 col0\" >0.263245</td>\n",
              "      <td id=\"T_30270_row3_col1\" class=\"data row3 col1\" >0.290161</td>\n",
              "      <td id=\"T_30270_row3_col2\" class=\"data row3 col2\" >0.286113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_30270_level0_row4\" class=\"row_heading level0 row4\" >prc_0 (mean)</th>\n",
              "      <td id=\"T_30270_row4_col0\" class=\"data row4 col0\" >0.955177</td>\n",
              "      <td id=\"T_30270_row4_col1\" class=\"data row4 col1\" >0.964194</td>\n",
              "      <td id=\"T_30270_row4_col2\" class=\"data row4 col2\" >0.962458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken to run external-validation: 206.30 seconds\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_csv('https://raw.githubusercontent.com/arturrur/mc853/refs/heads/using-KNN-imputer/data/test/teste19.csv')\n",
        "\n",
        "# impute missing values for df_test\n",
        "df_full_test = impute_missing(df_test)\n",
        "\n",
        "# evaluates\n",
        "evaluate_external(df_balanced, df_full_test);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}